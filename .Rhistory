library(here)
here()
here()
here("Figures")
library(rtdists)
library(sft)
library(here)
A <- 1   #accumulation starting point
b <- 1.4  #boundary threshold
t0 <- .3  #non-decision time
mean_v <- 1  #drift rate mean
sd_v <- 1  #drift rate sd
true_params <- c(A, b, t0, mean_v)  #create a vector for the true parameters
#Create a matrix for the true parameters
true_mat <- matrix(rep(true_params, each=n_iter), 10, 4)
library(rstan)
#Create a matrix for the true parameters
true_mat <- matrix(rep(true_params, each=n_iter), 10, 4)
#bias & precision
bias_n <- c()
prec_n <- c()
#sample sizes that will be tested
n_samp_vec <- c(50, 100, 200, 400)
#Outer Loop: different sample sizes
for (n_samples in n_samp_vec) {
#Middle Loop: different censoring levels
for (censoring in seq(.7, .95, by=.05)) {
## NOTE TO BRY: try changing what will be censored! eg., 0.05 to 0.3
#Create matrix for parameter estimates
all_params_n <- matrix(NA, n_iter, 4)
#Inner Loop: Repeating the actual simulation for each n_iter (estimate variance due to data)
for (i in 1:n_iter) {
#Use normal distributions of LBA drift rates to generate RTs
rt <- rlba_norm(n_samples, A, b, t0, mean_v, sd_v, posdrift = TRUE)
#Note: posdrift requires df rates to be pos
#Pull the RTs that would be censored
censored <- rt[,"rt"] > quantile(rt[,"rt"], censoring) #pulling from middle loop in steps of .05; T/F based on percentile (< or > than eg 70th percentile)
#Replace all censored RTs with the max RT from censoring limit
rt[censored,] <- matrix(rep(c(quantile(rt[,"rt"], censoring), 0), each=sum(censored)), sum(censored), 2)
##NOTE TO BRY: change code to censoring early tail instead of late; change < to > as needed in line 31
#Create a subset where data is correct/ censored, and all the uncensored disappears (aka: how bad are things?)
RT=rt[rt[,"response"]==1,]
#Pull the data we will fit with Stan model
stan_dat <- list(RT=RT, NUM_CHOICES=1, N=length(RT[,1])) #rt, num choice; num
#Stan Model (see GP_LBA file)
fit_optim2 <- mod_norm$optimize(data=stan_dat, iter=1E6)
#Pull parameter estimates from the Stan model
tmp_params <- c(fit_optim2$summary(c("A", "bMinusA", "tau", "v[1]"))[,2])$estimate
#Create a list  for the parameter estimates
##NOTE: why are we adding A & bMinusA?
all_params_n[i,] <- c(tmp_params[1], tmp_params[2] +tmp_params[1], tmp_params[3:4])
}
#Bias: Estimated - True with means
bias_n <- rbind(bias_n, c(n_samples, censoring, apply(all_params_n - true_mat, 2, mean)))
#Precision: Estimated - True with sd
prec_n <- rbind(prec_n, c(n_samples, censoring, apply(all_params_n - true_mat, 2, sd)))
}
}
n_iter <- 10  #should be on par for estimating var
#Outer Loop: different sample sizes
for (n_samples in n_samp_vec) {
#Middle Loop: different censoring levels
for (censoring in seq(.7, .95, by=.05)) {
## NOTE TO BRY: try changing what will be censored! eg., 0.05 to 0.3
#Create matrix for parameter estimates
all_params_n <- matrix(NA, n_iter, 4)
#Inner Loop: Repeating the actual simulation for each n_iter (estimate variance due to data)
for (i in 1:n_iter) {
#Use normal distributions of LBA drift rates to generate RTs
rt <- rlba_norm(n_samples, A, b, t0, mean_v, sd_v, posdrift = TRUE)
#Note: posdrift requires df rates to be pos
#Pull the RTs that would be censored
censored <- rt[,"rt"] > quantile(rt[,"rt"], censoring) #pulling from middle loop in steps of .05; T/F based on percentile (< or > than eg 70th percentile)
#Replace all censored RTs with the max RT from censoring limit
rt[censored,] <- matrix(rep(c(quantile(rt[,"rt"], censoring), 0), each=sum(censored)), sum(censored), 2)
##NOTE TO BRY: change code to censoring early tail instead of late; change < to > as needed in line 31
#Create a subset where data is correct/ censored, and all the uncensored disappears (aka: how bad are things?)
RT=rt[rt[,"response"]==1,]
#Pull the data we will fit with Stan model
stan_dat <- list(RT=RT, NUM_CHOICES=1, N=length(RT[,1])) #rt, num choice; num
#Stan Model (see GP_LBA file)
fit_optim2 <- mod_norm$optimize(data=stan_dat, iter=1E6)
#Pull parameter estimates from the Stan model
tmp_params <- c(fit_optim2$summary(c("A", "bMinusA", "tau", "v[1]"))[,2])$estimate
#Create a list  for the parameter estimates
##NOTE: why are we adding A & bMinusA?
all_params_n[i,] <- c(tmp_params[1], tmp_params[2] +tmp_params[1], tmp_params[3:4])
}
#Bias: Estimated - True with means
bias_n <- rbind(bias_n, c(n_samples, censoring, apply(all_params_n - true_mat, 2, mean)))
#Precision: Estimated - True with sd
prec_n <- rbind(prec_n, c(n_samples, censoring, apply(all_params_n - true_mat, 2, sd)))
}
}
library(cmdstanr)
library(rtdists)
library(sft)
library(here)
print(here)
paths = list(
sim = here("SimulationScripts"),
fig = here("Figures")
)
print(paths)
#mod <- cmdstan_model(file.path("../analysis/GP_LBA.stan"))
mod_cens <- cmdstan_model(file.path("../analysis/GP_LBA_diff2.stan"))
mod_norm <- cmdstan_model(file.path("../analysis/LBA.stan"))
a <- 3
b <- 10
a + b
# mod_cens <- cmdstan_model(file.path("../analysis/GP_LBA_diff2.stan"))
mod_norm <- cmdstan_model(here("SimulationScripts", "StanModels", "GP_LBA.stan"))
source("C:/Users/tfv563/Git/CensoringAccModels/SimulationScripts/Censored_LBA.R")
library(rtdists)
library(sft)
library(cmdstanr)
library(here)
n_iter <- 10  #should be on par for estimating var
A <- 1   #accumulation starting point
b <- 1.4  #boundary threshold
t0 <- .3  #non-decision time
mean_v <- 1  #drift rate mean
sd_v <- 1  #drift rate sd
true_params <- c(A, b, t0, mean_v)  #create a vector for the true parameters
#Create a matrix for the true parameters
true_mat <- matrix(rep(true_params, each=n_iter), 10, 4)
# mod_cens <- cmdstan_model(file.path("../analysis/GP_LBA_diff2.stan"))
mod_norm <- cmdstan_model(here("SimulationScripts", "StanModels", "GP_LBA.stan"))
#bias & precision
bias_n <- c()
prec_n <- c()
#sample sizes that will be tested
n_samp_vec <- c(50, 100, 200, 400)
#Outer Loop: different sample sizes
for (n_samples in n_samp_vec) {
#Middle Loop: different censoring levels
for (censoring in seq(.7, .95, by=.05)) {
## NOTE TO BRY: try changing what will be censored! eg., 0.05 to 0.3
#Create matrix for parameter estimates
all_params_n <- matrix(NA, n_iter, 4)
#Inner Loop: Repeating the actual simulation for each n_iter (estimate variance due to data)
for (i in 1:n_iter) {
#Use normal distributions of LBA drift rates to generate RTs
rt <- rlba_norm(n_samples, A, b, t0, mean_v, sd_v, posdrift = TRUE)
#Note: posdrift requires df rates to be pos
#Pull the RTs that would be censored
censored <- rt[,"rt"] > quantile(rt[,"rt"], censoring) #pulling from middle loop in steps of .05; T/F based on percentile (< or > than eg 70th percentile)
#Replace all censored RTs with the max RT from censoring limit
rt[censored,] <- matrix(rep(c(quantile(rt[,"rt"], censoring), 0), each=sum(censored)), sum(censored), 2)
##NOTE TO BRY: change code to censoring early tail instead of late; change < to > as needed in line 31
#Create a subset where data is correct/ censored, and all the uncensored disappears (aka: how bad are things?)
RT=rt[rt[,"response"]==1,]
#Pull the data we will fit with Stan model
stan_dat <- list(RT=RT, NUM_CHOICES=1, N=length(RT[,1])) #rt, num choice; num
#Stan Model (see GP_LBA file)
fit_optim2 <- mod_norm$optimize(data=stan_dat, iter=1E6)
#Pull parameter estimates from the Stan model
tmp_params <- c(fit_optim2$summary(c("A", "bMinusA", "tau", "v[1]"))[,2])$estimate
#Create a list  for the parameter estimates
##NOTE: why are we adding A & bMinusA?
all_params_n[i,] <- c(tmp_params[1], tmp_params[2] +tmp_params[1], tmp_params[3:4])
}
#Bias: Estimated - True with means
bias_n <- rbind(bias_n, c(n_samples, censoring, apply(all_params_n - true_mat, 2, mean)))
#Precision: Estimated - True with sd
prec_n <- rbind(prec_n, c(n_samples, censoring, apply(all_params_n - true_mat, 2, sd)))
}
}
#Create matrix for parameter estimates
all_params_n <- matrix(NA, n_iter, 4)
#Inner Loop: Repeating the actual simulation for each n_iter (estimate variance due to data)
for (i in 1:n_iter) {
#Use normal distributions of LBA drift rates to generate RTs
rt <- rlba_norm(n_samples, A, b, t0, mean_v, sd_v, posdrift = TRUE)
#Note: posdrift requires df rates to be pos
#Pull the RTs that would be censored
censored <- rt[,"rt"] > quantile(rt[,"rt"], censoring) #pulling from middle loop in steps of .05; T/F based on percentile (< or > than eg 70th percentile)
#Replace all censored RTs with the max RT from censoring limit
rt[censored,] <- matrix(rep(c(quantile(rt[,"rt"], censoring), 0), each=sum(censored)), sum(censored), 2)
##NOTE TO BRY: change code to censoring early tail instead of late; change < to > as needed in line 31
#Create a subset where data is correct/ censored, and all the uncensored disappears (aka: how bad are things?)
RT=rt[rt[,"response"]==1,]
#Pull the data we will fit with Stan model
stan_dat <- list(RT=RT, NUM_CHOICES=1, N=length(RT[,1])) #rt, num choice; num
#Stan Model (see GP_LBA file)
fit_optim2 <- mod_norm$optimize(data=stan_dat, iter=1E6)
#Pull parameter estimates from the Stan model
tmp_params <- c(fit_optim2$summary(c("A", "bMinusA", "tau", "v[1]"))[,2])$estimate
#Create a list  for the parameter estimates
##NOTE: why are we adding A & bMinusA?
all_params_n[i,] <- c(tmp_params[1], tmp_params[2] +tmp_params[1], tmp_params[3:4])
}
#Pull the RTs that would be censored
censored <- rt[,"rt"] > quantile(rt[,"rt"], censoring) #pulling from middle loop in steps of .05; T/F based on percentile (< or > than eg 70th percentile)
#Replace all censored RTs with the max RT from censoring limit
rt[censored,] <- matrix(rep(c(quantile(rt[,"rt"], censoring), 0), each=sum(censored)), sum(censored), 2)
rt
#Create a subset where data is correct/ censored, and all the uncensored disappears (aka: how bad are things?)
RT=rt[rt[,"response"]==1,]
rt
#Pull the data we will fit with Stan model
stan_dat <- list(RT=RT, NUM_CHOICES=1, N=length(RT[,1])) #rt, num choice; num
#Stan Model (see GP_LBA file)
fit_optim2 <- mod_norm$optimize(data=stan_dat, iter=1E6)
stan_dat
#Stan Model (see GP_LBA file)
fit_optim2 <- mod_norm$optimize(data=stan_dat, iter=1E6)
View(stan_dat)
source("C:/Users/tfv563/Git/CensoringAccModels/SimulationScripts/Censored_LBA.R")
library(cmdstanr)
n_iter <- 10  #should be on par for estimating var
A <- 1   #accumulation starting point
b <- 1.4  #boundary threshold
t0 <- .3  #non-decision time
mean_v <- 1  #drift rate mean
sd_v <- 1  #drift rate sd
true_params <- c(A, b, t0, mean_v)  #create a vector for the true parameters
#Create a matrix for the true parameters
true_mat <- matrix(rep(true_params, each=n_iter), 10, 4)
# mod_cens <- cmdstan_model(file.path("../analysis/GP_LBA_diff2.stan"))
mod_norm <- cmdstan_model(here("SimulationScripts", "StanModels", "GP_LBA.stan"))
source("C:/Users/tfv563/Git/CensoringAccModels/SimulationScripts/Censored_LBA.R")
View(stan_dat)
stan_dat$RT
censored
rt
RT
RT
